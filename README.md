# ğŸ”« Real-Time Dangerous Weapons Detection System

> **Production-ready weapon detection pipeline built in 3 months** â€” from initial prototype to deployment-ready models with 96%+ precision and recall.

<p align="center">
  <img src="https://img.shields.io/badge/Development-3%20Months-brightgreen" alt="Timeline"/>
  <img src="https://img.shields.io/badge/mAP50-95%25+-blue" alt="mAP50"/>
  <img src="https://img.shields.io/badge/FPS-30%2B-orange" alt="FPS"/>
  <img src="https://img.shields.io/badge/TensorRT-Optimized-76B900" alt="TensorRT"/>
</p>

---

## ğŸ“‹ Executive Summary

Complete end-to-end weapon detection system capable of detecting **guns, knives, rifles, and baseball bats** in real-time video streams. The system uses a two-stage architecture: person detection followed by weapon detection within person ROIs, dramatically reducing false positives while maintaining high recall.

### Key Achievements

| Metric | Value |
|--------|-------|
| **Development Time** | Oct 11, 2025 â†’ Jan 16, 2026 (~3 months) |
| **Classification Accuracy** | >96% precision & recall |
| **Detection mAP@50** | ~95% |
| **Real-time Performance** | 30+ FPS on 1080p (TensorRT) |
| **False Positive Reduction** | >90% vs baseline |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT VIDEO                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DEYO (RT-DETR) Person Detection                     â”‚
â”‚              â€¢ End-to-end transformer (no NMS overhead)          â”‚
â”‚              â€¢ 80 COCO classes, person = class 0                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ Person ROIs (expanded 15%)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              YOLO11-m Weapon Detection                           â”‚
â”‚              â€¢ 4 classes: knife, gun, rifle, baseball_bat        â”‚
â”‚              â€¢ Trained on curated 17k+ image dataset             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ByteTrack Temporal Filtering                        â”‚
â”‚              â€¢ 4-second memory buffer                            â”‚
â”‚              â€¢ 5-7 frame confirmation threshold                  â”‚
â”‚              â€¢ Eliminates flickering detections                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ANNOTATED OUTPUT                              â”‚
â”‚              â€¢ Red boxes: Guns/Rifles                            â”‚
â”‚              â€¢ Yellow boxes: Knives                              â”‚
â”‚              â€¢ Track IDs with confidence scores                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone <repository-url>
cd yolo_dangerous_weapons

# Pull model weights (Git LFS)
git lfs pull

# Install dependencies
pip install uv
uv sync
```

### Run Inference

```bash
cd inference
./run_simple.sh /path/to/video.mp4
```

### Export to TensorRT (Production)

```bash
cd export
python export_yolo.py \
    ../models/yolo/25_dec_2025_yolo11m/weights/best.pt \
    /workspace/exports \
    8 \
    640 \
    ../yolo_dataset_4_dec/data.yaml
```

---

## ğŸ“Š Development Timeline

A rapid iteration cycle with continuous improvements based on real-world testing.

```
Nov 13   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
         â”‚
         â”œâ”€ New DEYO + YOLO pipeline architecture
         â”‚
Nov 14   â”œâ”€ E2E inference scripts, initial experiments
         â”‚
Nov 24-27â”œâ”€ YouTube/GDD classification, experiment refinements
         â”‚
Dec 3    â”œâ”€ Captum model interpretability (GradCAM, Occlusion)
         â”‚
Dec 10   â”œâ”€ Confident Learning + DEYO integration
         â”‚  Label error detection with 5-fold cross-validation
         â”‚
Dec 12   â”œâ”€ Two rounds of confident learning cleanup
         â”‚  ~300 problematic images identified and fixed
         â”‚
Dec 20-25â”œâ”€ Major data curation sprint:
         â”‚  â€¢ SAM3 embedding outlier detection (870 outliers â†’ 20 removed)
         â”‚  â€¢ Hard negative mining (person crops, mobile phones, deployment false positives)
         â”‚  â€¢ Best recall achieved on private test set
         â”‚
Dec 29   â”œâ”€ TensorRT export optimization
         â”‚
Jan 1    â”œâ”€ DEYO ultralytics backend fixes for TRT
         â”‚
Jan 4    â”œâ”€ Webcam inference + temporal parameter tuning
         â”‚
Jan 6    â”œâ”€ Balanced dataset training, minority class improvement
         â”‚
Jan 7    â”œâ”€ Multi-GPU training setup, RF-DETR experiments
         â”‚
Jan 10-12â”œâ”€ RF-DETR training (74% mAP vs YOLO 70%)
         â”‚  Umbrella negatives added
         â”‚
Jan 13   â”œâ”€ Additional negative mining checkpoints
         â”‚
Jan 16   â”œâ”€ Final evaluation plots, attention visualization
         â”‚  Manifold studies for model understanding
         â”‚
         â–¼
       PRODUCTION READY
```

---

## ğŸ§ª State-of-the-Art Data Curation Pipeline

### 1. Confident Learning with 5-Fold Cross-Validation

Used **cleanlab** methodology with **ConvNeXTv2** classifier to systematically identify and fix label errors.

**Process:**
1. Cropped every weapon detection from all images
2. Split crops into 5 folds
3. Train ConvNeXTv2 on 4 folds, predict on held-out fold
4. Repeat for all folds (every image gets out-of-sample prediction)
5. Build confusion matrix using cleanlab, identify systematic mismatches
6. Manual review of flagged images

**Results:**
- Identified gun/rifle/baseball bat confusions
- Removed ambiguous and mislabeled images
- Two complete rounds of cleanup performed

```python
# confident_learning/convnext/train_convnext_cv_folds.py
# Automated 5-fold CV training with prediction collection
```

### 2. ConvNeXT Classifier for Verification

Trained a **ConvNeXTv2-tiny** classifier to verify weapon detections.

**Classes:** gun, knife, mobile phone, humans

**Performance:** >96% precision and recall

**Use Cases:**
- Post-detection filtering
- Label verification during data curation
- Confidence boosting for edge cases

### 3. SAM3 Embedding Outlier Detection

Novel approach using **Segment Anything Model 3** vision encoder for anomaly detection.

**Process:**
1. Extract SAM3 embeddings (mean-pooled) for all 17k images
2. Compute distance metrics in embedding space
3. Identify 870 statistical outliers
4. Manual review of outliers and their 20 nearest neighbors

**Results:**
- Found 15 highly problematic images
- Retrieved similar images for consistency check
- Removed ~20-25 truly problematic samples

### 4. Hard Negative Mining

Systematically reduced false positives through multi-source negative mining.

**Sources:**
- **~2000 person crops** from public datasets (non-weapon carrying individuals)
- **400 mobile phone images** (commonly confused with weapons)
- **Deployment false positives** â€” real-world false positives captured during actual system testing

**Impact:** Dramatically reduced false positives on umbrellas, sticks, phones, and other elongated objects

### 5. Monte Carlo Influence Functions

Advanced technique to identify which training images most impact model performance.

```python
# monte_carlo_influence/monte_carlo_influence.py
# 20 runs Ã— random subsets â†’ influence score per image
```

**Metrics tracked:** precision, recall, F1, mAP50, mAP50-95 at epochs 50 & 100

---

## ğŸ”¬ Model Interpretability Suite

Comprehensive tools to understand model decisions â€” critical for security applications.

### GradCAM Visualization

```bash
cd model_interp
./grad_cam.sh
```

Generates heatmaps showing which image regions drive detections.

### Integrated Gradients

```bash
./integrated_gradients.sh
```

Attribution method for understanding feature importance with smoothgrad noise reduction.

### Occlusion Sensitivity

```bash
cd captum
python weapon_occlusion.py --crops ./crops --out ./output
```

Sliding window occlusion to identify critical regions for each detection.

---

## ğŸ‹ï¸ Training Infrastructure

### YOLO11 Training

```bash
cd train/yolo
python train_yolo.py
```

**Features:**
- Multi-GPU DDP support
- Custom Albumentations augmentation pipeline
- Motion blur, defocus, ISO noise simulation
- Image compression artifacts
- Random shadows and brightness

### RF-DETR Training (Transformer Alternative)

```bash
cd train/rf-detr
./train_rfdetr.sh
```

**Comparison:**

| Model | Architecture | mAP | Small Objects | Training Time |
|-------|-------------|-----|---------------|---------------|
| YOLO11-m | CNN | ~70% | Good | 2-3 hours |
| RF-DETR Nano | Transformer | ~74% | Excellent | 8-12 hours |

### ConvNeXT Classifier Training

```bash
cd train/convnext
./train_convnext.sh
```

Uses Hugging Face Transformers with custom augmentation pipeline.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ inference/                    # Production inference scripts
â”‚   â”œâ”€â”€ person_weapon_simple.py   # Main two-stage pipeline
â”‚   â”œâ”€â”€ weapon_detector_subprocess.py  # GPU subprocess for YOLO
â”‚   â”œâ”€â”€ webcam_inference.py       # Real-time webcam demo
â”‚   â””â”€â”€ run_simple.sh             # Quick start wrapper
â”‚
â”œâ”€â”€ train/                        # Training pipelines
â”‚   â”œâ”€â”€ yolo/                     # YOLO11 training
â”‚   â”œâ”€â”€ rf-detr/                  # RF-DETR transformer training
â”‚   â””â”€â”€ convnext/                 # ConvNeXTv2 classifier
â”‚
â”œâ”€â”€ confident_learning/           # Data quality tools
â”‚   â”œâ”€â”€ yolo/                     # YOLO label error detection
â”‚   â””â”€â”€ convnext/                 # Classifier-based cleaning
â”‚
â”œâ”€â”€ model_interp/                 # Interpretability
â”‚   â”œâ”€â”€ grad_cam.py               # GradCAM visualization
â”‚   â”œâ”€â”€ integrated_gradients.py   # Attribution analysis
â”‚   â””â”€â”€ guided_gradcam.py         # Guided GradCAM
â”‚
â”œâ”€â”€ captum/                       # Feature attribution
â”‚   â”œâ”€â”€ weapon_occlusion.py       # Occlusion sensitivity
â”‚   â””â”€â”€ extract_person_crops.py   # Crop extraction utility
â”‚
â”œâ”€â”€ monte_carlo_influence/        # Influence functions
â”‚   â””â”€â”€ monte_carlo_influence.py  # Training influence analysis
â”‚
â”œâ”€â”€ outliers/                     # Outlier detection
â”‚   â”œâ”€â”€ knn_outlier.ipynb         # KNN-based outlier detection
â”‚   â””â”€â”€ sam_3_embeddings.ipynb    # SAM3 embedding analysis
â”‚
â”œâ”€â”€ evals/                        # Evaluation scripts
â”‚   â”œâ”€â”€ eval_full_test.py         # Full test set evaluation
â”‚   â”œâ”€â”€ eval_dangerous_test.py    # Dangerous subset eval
â”‚   â””â”€â”€ evaluate_convnext.py      # Classifier evaluation
â”‚
â”œâ”€â”€ export/                       # Model export
â”‚   â”œâ”€â”€ export_yolo.py            # TensorRT export
â”‚   â””â”€â”€ deyo_export.py            # DEYO export
â”‚
â”œâ”€â”€ DEYO/                         # RT-DETR person detector
â”‚   â””â”€â”€ ultralytics/              # Custom ultralytics fork
â”‚
â”œâ”€â”€ notebooks/                    # Research notebooks
â”‚   â”œâ”€â”€ attention_viz.ipynb       # Attention visualization
â”‚   â”œâ”€â”€ manifold.ipynb            # Embedding manifold analysis
â”‚   â””â”€â”€ rf_detr.ipynb             # RF-DETR experiments
â”‚
â””â”€â”€ docs/                         # Documentation
    â”œâ”€â”€ PERSON_WEAPON.md          # Pipeline architecture
    â””â”€â”€ temporal_filtering_experiments.md
```

---

## âš¡ Performance Benchmarks

### Inference Speed (RTX 4090)

| Precision | Latency/Frame | FPS | Use Case |
|-----------|--------------|-----|----------|
| FP32 | ~30-40ms | 25-33 | Maximum accuracy |
| FP16 | ~15-25ms | 40-65 | **Default (recommended)** |
| INT8 | ~10-20ms | 50-100 | Edge deployment |

### Temporal Filtering Impact

| Memory Buffer | Inference Time | Dropped Detections |
|--------------|----------------|-------------------|
| 1 second | 18.1ms | 0.3% |
| 2 seconds | 17.5ms | 0.3% |
| 4 seconds | 17.8ms | 0.2% |

**Recommended settings:**
```bash
--track --track_persist 120 --min_hits 5
```

---

## ğŸ”§ Troubleshooting

| Issue | Solution |
|-------|----------|
| Models not loading | `git lfs pull` |
| GPU OOM during training | Reduce batch size, use gradient accumulation |
| False positives | Use person+weapon pipeline, increase `--min_hits` |
| Slow inference | Use INT8 TensorRT engine, increase `--downscale` |
| Module conflicts (DEYO/YOLO) | Subprocess architecture handles this automatically |

---

## ğŸ“š Technical References

- **YOLO11**: Ultralytics latest detection architecture
- **DEYO/RT-DETR**: Real-Time Detection Transformer (end-to-end, no NMS)
- **RF-DETR**: Roboflow Detection Transformer
- **ConvNeXTv2**: Facebook's modernized ConvNet
- **cleanlab**: Confident learning for label error detection
- **SAM3**: Segment Anything Model for embeddings
- **ByteTrack**: Simple and effective multi-object tracking
- **Captum**: PyTorch model interpretability library

---

## ğŸ“„ Requirements

- Python 3.12+
- CUDA GPU (16GB+ VRAM for training)
- TensorRT 8.6+ (for optimized inference)
- 20GB+ disk space for model exports

---

## ğŸ“ Contact

For questions about implementation details or deployment assistance, please reach out.

---

<p align="center">
  <i>Built with â¤ï¸ using SOTA deep learning techniques</i>
</p>
