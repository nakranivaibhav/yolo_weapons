# Detection-Only Pipeline (YOLO TensorRT)
**Tiled Detection + ROI Refinement + ByteTrack**

---

## ğŸ“Š Pipeline Overview

### Pipeline Flow (Detection-Only Approach)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   4K Video      â”‚
â”‚   3840Ã—1608     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Downscale 0.5x â”‚
â”‚   1920Ã—804      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tile into       â”‚
â”‚ overlapping     â”‚
â”‚ 640Ã—640 pieces  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YOLO Detection  â”‚
â”‚ (all tiles)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SAHI NMS Merge  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ROI Refinement  â”‚
â”‚ (OPTIONAL)      â”‚
â”‚ Re-detect on    â”‚
â”‚ full-res crops  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ByteTrack     â”‚
â”‚ Multi-object    â”‚
â”‚   Tracking      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Visualization   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Difference from Two-Stage Pipeline

### Detection-Only vs Two-Stage

**This Pipeline (Detection-Only):**
```
YOLO Detection â†’ SAHI NMS â†’ ROI Refinement (YOLO again) â†’ ByteTrack
```

**Two-Stage Pipeline:**
```
YOLO Detection â†’ SAHI NMS â†’ ConvNeXT Classification â†’ ByteTrack
```

### Advantages
âœ… **Simpler** - Single model (YOLO only)
âœ… **Faster** - No separate classification model
âœ… **Lower memory** - One model loaded
âœ… **Good for gun/knife detection** when YOLO is well-trained

### Disadvantages
âŒ **Less refinement** - No specialized classifier
âŒ **More false positives** - YOLO can confuse similar objects
âŒ **Limited to YOLO classes** - Can't add post-detection verification

---

## 1ï¸âƒ£ Tiled Detection Strategy

### Dynamic Tiling with Overlap

Unlike the hardcoded 8-tile approach, this uses **dynamic tiling** based on overlap parameter:

```python
def create_tiles(img_w, img_h, tile_size, overlap):
    stride = tile_size - overlap
    tiles = []
    
    for y in range(0, img_h, stride):
        for x in range(0, img_w, stride):
            x1, y1 = x, y
            x2 = min(x + tile_size, img_w)
            y2 = min(y + tile_size, img_h)
            
            # Skip tiny edge tiles
            if (x2 - x1) < tile_size // 2 or (y2 - y1) < tile_size // 2:
                continue
            
            tiles.append((x1, y1, x2, y2))
```

### Overlap Calculation

**For 1920Ã—804 with tile_size=640, overlap=128:**

```
stride = 640 - 128 = 512 pixels

Horizontal tiles:
x=0:     0 to 640
x=512:   512 to 1152  (128px overlap with previous)
x=1024:  1024 to 1664 (128px overlap)
x=1536:  1536 to 1920 (partial tile, might be skipped)

Vertical tiles:
y=0:     0 to 640
y=512:   512 to 804   (partial height, might be skipped)

Result: ~6-8 tiles per frame
```

### Overlap Percentage
```
overlap_px = 128
tile_size = 640
overlap_% = (128 / 640) * 100 = 20%
```

**Why 20% overlap?**
- Ensures objects at tile boundaries get detected
- Lower than 33% used in 2-stage pipeline
- Fewer total tiles = faster processing

---

## 2ï¸âƒ£ YOLO Detection

### Process Flow

```
1. Create tiles from downscaled frame (1920Ã—804)
2. Extract tile images
3. Run YOLO on all tiles
   - Sequential: One tile at a time
   - Batched: All tiles in one forward pass
4. Convert tile-local coords to global coords
5. Store all detections with confidence scores
```

### Coordinate Transformation

```
Tile coordinates (local):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tile 2 starts at (512, 0)
Detection in tile: (100, 200, 150, 300)

Global coordinates:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Add tile offset: (100+512, 200+0, 150+512, 300+0)
Result: (612, 200, 662, 300)
```

### Batch vs Sequential

**Sequential Processing:**
```python
for tile in tiles:
    result = model(tile, imgsz=640, conf=0.25)
    # Process each tile separately
```
- Simpler
- Lower memory
- Slower (multiple forward passes)

**Batch Processing:**
```python
all_results = model(tiles, imgsz=640, conf=0.25)
# Process all tiles in one forward pass
```
- Faster (1 forward pass)
- Better GPU utilization
- Requires more VRAM

---

## 3ï¸âƒ£ SAHI NMS (Same as Two-Stage)

### Purpose
Merge duplicate detections from overlapping tiles

### Algorithm
```
For each pair of detections:
  1. Calculate IoU (Intersection over Union)
  2. If IoU > threshold (0.45):
     - Consider them as same object
     - Keep detection with higher confidence
     - Discard lower confidence detection
  3. Repeat until no more merges
```

### Example

```
Before SAHI NMS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tile 1: gun at (100, 200, 150, 300), conf=0.85
Tile 2: gun at (612, 200, 662, 300), conf=0.82  â† Same gun!
Tile 3: knife at (1200, 400, 1280, 520), conf=0.91

After SAHI NMS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
gun at (100, 200, 150, 300), conf=0.85
knife at (1200, 400, 1280, 520), conf=0.91

Reduction: 3 â†’ 2 detections
```

---

## 4ï¸âƒ£ ROI Refinement (Optional)

### What is ROI Refinement?

Instead of using a **separate classifier**, we **re-run YOLO** on full-resolution crops around detections.

### Why ROI Refinement?

```
Problem with Downscaled Detection:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Original: 3840Ã—1608
Downscaled: 1920Ã—804 (50% size)

Small objects become even smaller!
Details are lost in downscaling.

Solution:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Detect on downscaled (fast, finds general location)
2. Crop from FULL-RES original frame
3. Re-detect on full-res crop (accurate, sees details)
```

### ROI Refinement Process

```
Step 1: Get merged detection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Detection: gun at (100, 200, 150, 300) @ 1920Ã—804

Step 2: Scale to full resolution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Full-res coords: (200, 400, 300, 600) @ 3840Ã—1608

Step 3: Expand ROI for context
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Original box: 100Ã—200 pixels
Expand by 20%: +20px on each side
Expanded: (180, 380, 320, 620)

Step 4: Crop from original frame
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crop = frame_orig[380:620, 180:320]
Size: 140Ã—240 pixels (full resolution)

Step 5: Re-run YOLO on crop
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
result = YOLO(crop, conf=0.30)

Step 6: Verify detection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
If YOLO detects object in crop:
  âœ“ Keep detection (verified)
  Update coordinates and confidence
Else:
  âœ— Reject detection (false positive)
```

### Refinement Parameters

**`--roi_expand=0.2`** (20% expansion)
```
Gives context around object
Too small: Might crop object edges
Too large: Includes irrelevant background
```

**`--refine_conf=0.30`** (lower threshold)
```
Lower than initial detection (0.50)
Allows re-detection in cropped context
Higher confidence = fewer false positives
```

### Example Workflow

```
Frame arrives
     â†“
Downscale to 1920Ã—804
     â†“
Detect: 3 detections
     â†“
SAHI NMS: 2 detections
     â†“
ROI Refinement:
  Detection 1:
    - Crop from 3840Ã—1608
    - YOLO re-detects â†’ âœ“ Verified
    - Update coords & conf
  
  Detection 2:
    - Crop from 3840Ã—1608
    - YOLO finds nothing â†’ âœ— Rejected
     â†“
Final: 1 verified detection
```

### Performance Impact

```
Without ROI Refinement:
  Detection: 20-25ms
  Total: 25-30ms
  
With ROI Refinement:
  Detection: 20-25ms
  ROI crops: 2-3 detections
  Re-detection: 5-10ms (batch of crops)
  Total: 30-40ms
```

---

## 5ï¸âƒ£ ByteTrack (Same as Two-Stage)

### Tracking Flow

```
Frame N:   gun at (100, 200) â†’ Assign ID=1
Frame N+1: gun at (105, 205) â†’ Match to ID=1 (same gun, moved)
Frame N+2: gun at (110, 210) â†’ Match to ID=1
Frame N+3: no detection      â†’ ID=1 still alive (persist)
Frame N+4: gun at (115, 215) â†’ Match to ID=1 (re-identified)
```

### Key Parameters

**`--min_hits=3`**
```
Track must be detected 3 consecutive times to appear
Prevents brief false positives from showing
```

**`--track_persist=30`**
```
Keep track alive for 30 frames after last detection
Handles temporary occlusions
At 30 FPS = 1 second of persistence
```

**`--match_thresh=0.8`**
```
IoU threshold for matching detection to track
0.8 = Very strict (prevents ID switches)
```

---

## 6ï¸âƒ£ Complete Pipeline Comparison

### Timeline: Detection-Only

```
T=0ms   : Frame arrives (3840Ã—1608)
T=1ms   : Downscale to 1920Ã—804
T=2ms   : Create ~8 tiles (640Ã—640)
T=5ms   : YOLO detection on all tiles
T=25ms  : SAHI NMS merge
T=27ms  : Extract ROIs (if refinement enabled)
T=30ms  : YOLO re-detection on crops
T=40ms  : ByteTrack update
T=42ms  : Visualization
T=45ms  : Frame complete âœ“

Target: < 33.3ms @ 30 FPS
Actual: ~45ms (MARGINAL)
```

### Timeline: Two-Stage (for comparison)

```
T=0ms   : Frame arrives (3840Ã—1608)
T=1ms   : Downscale to 1920Ã—804
T=2ms   : Create 8 tiles (640Ã—640)
T=5ms   : YOLO detection (batch=8)
T=25ms  : SAHI NMS merge
T=27ms  : Extract ROIs
T=30ms  : ConvNeXT classification
T=38ms  : ByteTrack update
T=40ms  : Visualization
T=42ms  : Frame complete âœ“

Target: < 33.3ms @ 30 FPS
Actual: ~35-42ms (REAL-TIME)
```

---

## ğŸ“ˆ Performance Comparison

### Detection-Only Pipeline

**Pros:**
```
âœ… Single model (YOLO only)
âœ… Simpler deployment
âœ… Lower memory footprint
âœ… No classifier training needed
```

**Cons:**
```
âŒ Slower (45ms vs 35ms)
   - ROI refinement adds 10-15ms
   - Re-running YOLO is expensive
âŒ Less accurate
   - No specialized gun/knife classifier
   - More false positives
```

### Two-Stage Pipeline

**Pros:**
```
âœ… Faster (35ms vs 45ms)
   - ConvNeXT is lighter than YOLO
   - Batch classification efficient
âœ… More accurate
   - Specialized classifier
   - 98%+ gun/knife accuracy
âœ… Better filtering
   - Strict classification threshold
   - Reduces false positives
```

**Cons:**
```
âŒ More complex
   - Two models to manage
   - Classifier training required
âŒ Higher memory
   - YOLO + ConvNeXT loaded
```

---

## ğŸ”§ Configuration

### Detection-Only Script

```bash
python tiled_tensorrt_realtime.py \
  --video video.mp4 \
  --model yolo11s_640.engine \
  --tile_size 640 \
  --overlap 128 \
  --conf 0.25 \
  --iou 0.45 \
  --camera_fps 30 \
  --downscale 0.5 \
  --batch_tiles \              # Batch process tiles
  --refine_rois \              # Enable ROI refinement
  --roi_expand 0.2 \           # 20% expansion
  --refine_conf 0.30 \         # Lower conf for refinement
  --track \                     # Enable tracking
  --min_hits 3 \               # Anti-flicker
  --track_persist 30 \         # 1 second @ 30fps
  --save_vis                   # Save video
```

### Two-Stage Script (for comparison)

```bash
python tiled_classification_realtime.py \
  --video video.mp4 \
  --detect_model yolo11s_640.engine \
  --classify_model convnext.ts \
  --tile_size 640 \
  --detect_batch 8 \
  --classify_batch 4 \
  --conf 0.50 \                # Higher YOLO threshold
  --classify_conf 0.90 \       # Strict classifier
  --iou 0.45 \
  --camera_fps 30 \
  --downscale 0.5 \
  --classify_rois \            # Use classifier instead
  --track \
  --min_hits 5 \               # Stricter anti-flicker
  --track_persist 45           # Longer persistence
```

---

## ğŸ“Š When to Use Each Approach

### Use Detection-Only When:

âœ… **Simple deployment needed**
- Single model easier to manage
- Limited resources

âœ… **YOLO is highly accurate**
- Well-trained on your specific classes
- Low false positive rate already

âœ… **Speed not critical**
- Can tolerate 40-50ms latency
- 20-25 FPS acceptable

âœ… **Memory constrained**
- Can't load two models
- Embedded devices

### Use Two-Stage When:

âœ… **Maximum accuracy required**
- Gun vs knife distinction critical
- False positives unacceptable

âœ… **Real-time required**
- Need 30+ FPS consistently
- < 33ms latency target

âœ… **Classification expertise**
- Can train specialized classifier
- Have labeled crop dataset

âœ… **Resources available**
- GPU memory for two models
- Can deploy complex pipeline

---

## ğŸ¬ Visual Comparison

### Detection-Only Flow
```
VIDEO FRAME
     â†“
  Downscale
     â†“
  Tile (8x)
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLO   â”‚ â† Single model
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  SAHI NMS
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLO   â”‚ â† Same model again (refinement)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  ByteTrack
     â†“
  OUTPUT
```

### Two-Stage Flow
```
VIDEO FRAME
     â†“
  Downscale
     â†“
  Tile (8x)
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOLO   â”‚ â† Detection model
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  SAHI NMS
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ConvNeXT â”‚ â† Different classification model
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  ByteTrack
     â†“
  OUTPUT
```

---

## ğŸ’¡ Key Takeaways

### Detection-Only Pipeline

**Best for:**
- Prototyping and testing
- Simple deployments
- When YOLO accuracy is sufficient
- Resource-constrained environments

**Characteristics:**
- **Simpler**: One model to manage
- **Slower**: ROI refinement is expensive
- **Less accurate**: No specialized classification
- **Lower memory**: Single model loaded

### Recommendation

**Start with Detection-Only** to:
- Validate the tiling approach
- Understand performance characteristics
- Identify false positive patterns

**Upgrade to Two-Stage** when:
- Detection-only has too many FPs
- Need gun/knife distinction accuracy
- Speed becomes critical
- Resources are available

---

## ğŸ”„ Migration Path

### From Detection-Only to Two-Stage

1. **Collect crop dataset**
   - Save ROI crops from detection-only pipeline
   - Label as gun/knife
   - 2000+ samples recommended

2. **Train ConvNeXT classifier**
   ```bash
   python train_convnext.py \
     --data crops/ \
     --epochs 20
   ```

3. **Export classifier**
   ```bash
   python export_torch_compile.py \
     --model_path checkpoint/ \
     --batch_size 4
   ```

4. **Switch scripts**
   - Replace `tiled_tensorrt_realtime.py`
   - With `tiled_classification_realtime.py`
   - Update config parameters

5. **Tune thresholds**
   - Start with `--conf 0.50`
   - Set `--classify_conf 0.90`
   - Adjust based on results

---

**Documentation for legacy detection-only approach using single YOLO model with optional ROI refinement**

